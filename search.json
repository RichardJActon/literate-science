[
  {
    "objectID": "index.html#literate-science",
    "href": "index.html#literate-science",
    "title": "Literate Science",
    "section": "0.1 Literate Science",
    "text": "0.1 Literate Science\nA new paradigm for academic publishing\nThis is currently very much a work in progress and may change dramatically and rapidly, some sections may be drafts, outlines and bullet points to be filled in and properly edited as I go"
  },
  {
    "objectID": "index.html#summary",
    "href": "index.html#summary",
    "title": "Literate Science",
    "section": "0.2 Summary",
    "text": "0.2 Summary\nI have two main proposals:\nLiterate Publishing - An Open Source academic authoring, publishing and review platform which provides reproducible analyses by linking data, computation and results\nReview Bounties - a new business model for academic publishers\n\n0.2.1 Literate Publishing\n\n\n0.2.2 Review Bounties"
  },
  {
    "objectID": "001-intro.html#footnotes",
    "href": "001-intro.html#footnotes",
    "title": "2  Introduction",
    "section": "",
    "text": "Now defunct? Dead links https://www.reddit.com/r/TokenEUREKA/, https://doi.org/10.1038/d41586-019-00447-9, https://medium.com/eureka-token/stories-can-wait-science-cant-33810dd17c9f↩︎\nNotion - GPL like agreement for lab protocols? Copy the obligation to make changes available to the upstream, and encourage upstreaming them yourself by publishing your protocols under the same licence. If you use a protocol and cite it you must describe your modifications to it and release them under the same terms as the original. enforcibility? comercial incentives might limit adoption due to regressive attitudes to IP control rather than emphasis on convenience and quality of service↩︎"
  },
  {
    "objectID": "002-units_of_publication.html#the-new-hierarchy-of-publication-types",
    "href": "002-units_of_publication.html#the-new-hierarchy-of-publication-types",
    "title": "3  Units of publication",
    "section": "3.1 The new hierarchy of publication types",
    "text": "3.1 The new hierarchy of publication types\nThe traditional hierarchy of scientific publications looks something like this:\n\nPrimary papers\nReviews\nTextbooks\n\nRunning from the narrowest to the broadest scope and most detail to the highest level of synthesis and abstraction, in theory. This is an oversimplification and we have been adding to the ontology of publication types somewhat in recent years, more on this later. Some additions such as pre-registrations have caught on more in some field that others. Whilst some innovations are reasonably domain specific others could benefit from being more widely adopted.\nIn addition to the mainline scientific communication hierarchy there is of course the communication of science to a broader audience. Ranging from cross disciplinary communication to communicating with the public, policy makers, and in education. These essential functions of the scientific community with immense civic import that are undervalued in the current model.\nIntegrating many of the new forms of publication that have arisen I would propose a new more granular hierarchy:\n\npre-registrations / experimental plans/designs\nData, Methods, Protocols, Pipelines, & Software publications\nExperimental results\nTheory, synthesis and prediction (without a requirement for experimental detail but ideally with resolution criteria)\nTopic Reviews, Curated Best practice resources for lab protocols and computational pipelines as well as, Benchmarking resources\n“git-books” - much like a textbook but under continuous revision\nScience communication (multimedia, curricula, policy briefs etc.)\n\nWe have made some progress towards this model already, everything that I list here already exists in some form after all. We have however not really thought or talked about them much when put together as a whole - at least not in my experience. Bringing this more granular view of the unit scientific publication to the fore I think has a number of advantages."
  },
  {
    "objectID": "002-units_of_publication.html#problems-with-the-current-units-of-publication-and-how-the-new-ones-address-them",
    "href": "002-units_of_publication.html#problems-with-the-current-units-of-publication-and-how-the-new-ones-address-them",
    "title": "3  Units of publication",
    "section": "3.2 Problems with the current units of publication and how the new ones address them",
    "text": "3.2 Problems with the current units of publication and how the new ones address them\nModern publications particularly in the life sciences (my home territory) are too long, too complex, too narrative driven and, in partial contradiction with being too long are often too compressed. I’ll be expanding on these points, but for now take my word for it that this is not merely me complaining about trying to read 50 page cell papers and still have the time to do anything else. As the ability to do experimental work has increased due to technological advances that have rendered some projects that would have taken decades 20-30 years ago achievable in months the expectations for publications have changed. Reading papers from the 1980’s and 90’s one is struck by their relative shortness and simplicity compared to many modern papers. Indeed, empirically scientific manuscripts are getting harder to read (Plavén-Sigray et al. 2017).\nTODO Check the data on length, has it actually increased?, analyse the pubmed full text dataset\nI would argue that is this mostly not due to an intrinsic increase in the complexity of the systems we are able to study and the methods that we are using to study them. These have increased and some of the added difficulty may be accounted for by the increasing number and narrowness of specialties but I think more blame is due to failure to adapt institutional structures around ever more and narrower specialties as well as publication practices.\nAnnecdotally it is my impression that the number of experiments and the technical complexity of those experiments per paper has been on the rise (I would welcome empirical evidence from literature mining on this subject to confirm or disconfirm my conjecture here). It is marvelous that we can get more done and that we have sophisticated new tools with which to work, however this presents a number of problems when publishing.\n\n3.2.1 The minimum unit of publication\nLet us step back for a moment a consider what is the minimum unit of publication? According to the proponents of ‘nanopublications’ this minimal unit of publication is a simple statement consisting of a subject, object predicate triple that is generated by some author and asserted by some party (Groth, Gibson, and Velterop 2010).\n\ne.g. the journal of the widely understood asserts that the statement “Malaria is transmitted by mosquitoes” was authored by John Smith.\n\nThis minimal unit of publication structured in this fashion has a number of very useful properties for automating the creation of a web of semantic meaning in the scientific literature. nanopublications have an Resource Description Framework (RDF) graph structure conformant with W3C standards for a semantic web which provide a formal structure to publication metadata which would massively improve the transparency of the structure of semantically meaningful relationships in the published scientific literature to automated analyses. Such structure has considerable potential to increase the ease and effectiveness of literature mining and may permit semi-automated means of assessing the weight of evidence supporting a particular statement and identifying gaps and weaknesses it the current state of understanding of particular topics. 1\nHowever, it would be somewhat cumbersome and impractical to write only in nano-publications, whilst they may be convenient for a machine legibility they are less convenient for human legibility. I envisage them catching on once at least semi-automated tools can convert a more human unit of publication into a collection of nano-publications. Thus my proposal is not to jump directly to nanopublications but rather to adopt micropublications, a minimal unit of publication on a more human scale.\n\n\n3.2.2 The Human Unit of Publication\nThe scale I propose is approximately that of a single experiment when publishing results, with the scope getting a little harder to delineate for some other types of publication. Where now you publish a single paper you would instead publish a series of micropublications. You might start with an outline of the question that you are seeking to address defining the experiment(s) you would like to perform and the predictions you make for their outcomes. You would then follow up with a number of micropublications detailing results of individual experiments and lastly with a synthesis publication which draws together your results into an explanatory narrative. This series of publications need not come exclusively from your research group, you might collaborate with others, with individual experiments being predominantly carried out and authored by other groups or simply making use of the results of others which make the desired point and citing them or collaborating with them when writing your synthesis. I contend that this way of working would have a number of major advantages over the current publication model, which I will now outline of greater detail.\nFirst publishing an experimental question micropublication can serve as a preregistration of your study which will later allow a clear delineation between advanced predictions and post-hoc analyses that is often lacking under current publishing norms. In addition the process defining your question and plan with sufficient clarity to publish it and to solicit feedback on this plan from other domain experts is an extremely useful exercise for catching and fixing things like issues with the experimental design, potential technical problems, and lack of clarity in your question. I’d bet an alarming amount of resources are wasted as a results of imperfectly conceived experiments performed by researchers who have not subjected their plans to adequate advanced scrutiny. This approach shortens the feedback loop. It also leverages a social psychological pressure to generate something of high quality when doing it in public this serves as a commitment device to produce well defined, high quality experimental plans of a standard you might not reach if you are only making them for your own internal consumption.\nOf course not all work can be readily subject to this process. Exploratory analysis that reveals interesting and unexpected things is important, useful and by it’s nature is not readily subject to pre-registration. The point is not to de-value this work but to much more effectively differentiate it from formally predictive hypothesis testing which is an important statistical distinction that is not always easily discerned in the current format. A perfectly valid micropublication could be one which details interesting observations and potentially resultant predictions made from exploratory data analysis of data not explicitly generated to address the question now being asked of it.\nPublishing micropublications also provides a better credit attribution model than for larger papers as by its nature a smaller number of people are likely to contribute to a smaller publication. The lead author (in biological publishing conventions) may differ across a series of micropublications that if condensed to a single paper would see the bulk of the credit go to a single individual. Fewer co-authors also reduces collective action problem of choice to publish in alternate publication venues if you don’t know your co-authors that well you are more likely to assume that they will want to publish in conventional high impact journals and not be too keen on you deciding to publish in a less fashionable venue so we default to the conservative option of a conventional journal rather than risk getting into an extended email argument about the benefits of open science with collaborators who we want to keep happy. We are stuck in a bad nash equilibrium because of our assumptions about our co-authors and they may not be as valid as we conservatively expect.\nPublishing in this more piecemeal fashion serves as an antidote to two related problems that of excessive narrative seeking in long form publication and of the file draw effect. The incentive to have a nice clean tight narrative into which all of your results fit perfectly in order to publish a paper contributes to the file draw effect as people may be inclined not to publish the results of experiments which do not fit the narrative as well as they hoped. If you publish as you go you can’t decide to hold a result back that does not fit a story. If you are constructing the narrative from units that you have already published you are forced to address any conflicting data and engage with judging the relative merits of the available data and/or those of your proposed model.\nIt is also likely to lead to a higher number of publications of negative results as pre-registering an experiment carries greater incentives to follow it up with the results*\n\nIn addition this should also lead to the increased documentation of experiments which failed for technical reasons. This is a gap in the publication record that I suspect leads to a lot of wasted resources as many separate groups may have the same idea to test the seemingly obvious thing that turns out to be a lot more technically challenging than anticipated. If failed attempts were better documented it would facilitate groups iterating on different approaches to the problem rather than potentially independently hitting the same stumbling blocks. Increasing the degree to which informal knowledge of these sort of technical challenges is codified rather than remaining implicit in the oral traditions of individual research groups, as is quite commonplace, is a boon to future attempts at replicating experimental work.\nThe point of scientific culture is to create a pit of sucess for the truth seeking process, to establish a set of cultural norms which make it as inevitable as possible that our collective map of reality incrementally conforms ever more closely to the underlying territory. To make it hard for us to do anything other than our best work, the current publishing model is not a pit of success it is a pinnacle that you must strive to climb to produce high quality work. This is not to say that good science is not hard work quite the opposite. It is framing the publishing process as a design problem for how structure the incentives to make it easier to get rewarded for producing high quality work and very hard to get rewarded for low quality work. In costly signaling theory terms it’s making the signal of getting a peer reviewed publication very tightly correspond to high quality work. At present the quality of the signal is a little degraded, it is possible to put forth a lesser effort orthogonal to the scientific merit of the work in order to get published, somewhat analogous to the classic textbook example of gluing on some longer tail feathers to Jackson’s widow birds, tail length ceases to be a good indicator of mate quality. Don’t get me wrong there are a lot of good papers out there but increasingly this seems to be in spite of and not because of the publication medium.\nIt is possible to split the readership of a typical primary research paper into two groups which I call technical and contextual readers. A technical reader is doing very similar work to that done in the paper and is interested in the detailed methodology of that paper as they may do something similar themselves. The contextual reader is primarily interested in the conclusions of the paper, and applying the understanding that it generated to thinking about their own related question. These two audiences frequently overlap in the same person but not always\nAs currently consititutes papers are frequently sub-optimally structured for both audiences\nsynthesis ~ review more optimal depth for the two audiances of a paper, the technical and the contextual push technical experimental detail down into the results micropublications with more high level considerations in the synthesis or only experimental details which are under dispute\nspeaks to the compression issue, gives the individual components space to breath and acctually provide adaquate technical details and does not make extracting a higher level understanding more difficult due to the inclusion of extraneous detail.\nFor an increasing number of papers the scope of what they investigate exceeds the reasonable ability of the typical number of reviewers to adequately assess the material. Domain expertise in the system being studied, statistical expertise, methodological expertise in the methods employed are all needed for a rigorous assessment of most papers. The larger and more complex the unit of publication the greater the opportunity for diffusion of responsibility among the reviewers who may be free to assume that one of the others will give adequate scrutiny to aspects outside their direct expertise, this may not be the case. As papers cover an increasing number, and more complex experiments the probability of error in some part of the work increases, as the more things are in a unit of publication the more chances there are for at least one of them to be in error. I suspect that error increases at a rate greater than that which you would expect from simple conjunction, due in part to the diffusion of responsibility for review earlier hypothesized earlier. I would contend that the increasing complexity of papers contributes to the replication problems commonplace in many field in part because of the challenges of adequately reviewing them.\nI propose experimental tests of the diffusion of responsibility in reviewers and effectiveness of reduced publication size/scope at reducing it. For example one could send a manuscript with a statistical error to two types of sets of reviewers. one pool of reviewers none of whom have stats expertise and count how often to they a. catch the error, b. flag their lack of expertise and ask for review by a stats expert c. take no action. The second set of reviewers should contains a stats expert to establish a base rate for missing the error. Try this for a conventional publication and ‘micropublication’ containing essentially only the experiment with the error.\nThat addresses criticisms of too long and complex, now for the related problem of being what I think of as too compressed. The text of papers in nature for instance is almost always to short to adequately convey the necessary detail for the amount of work that has been done to justify a place in such a prestigious publication. It is a primary publication so must be precise and near exhaustive in its description of the work done\n\nThis greater specialization of publication types reduces citation dilution, increases reviewability by narrowing the scope of expertise necessary to evaluate smaller publications. It also pushes the making of narratives away from the primary research where it can contribute to the file draw problem and pushes it to it’s own ‘sense making’ layer. This also reduces time to publication it can take years to get things published under the current paradigm, this should tell us publications are too big and too complex and need to be broken into smaller parts to shorten the feedback loop of the review process.\nThis stack also enables professional scientific specialization not purely in the dimension of subject specialty but at level of analysis, you could specialize in sharpening up the theoretical models or experimental design in a broader array of disciplines publishing mostly at the theory level or generate a lot of high quality data. High level synthesis, curation and communication of important scientific discovery in the field as gitbook maintainers to keep the ‘textbooks’ current on an ongoing basis without the constraint of editions whilst remaining citable because of version control. Increases accessibility of publishing to students and non-domain experts making smaller contributions that are still in the public interest to have published. It may be argued that this approach increases the amount of ‘noise’ there are already so many publications that it is impossible to keep up in many fields, this is part of why new specialties are needed in the synthesis of experimental information into models. It is only possible for individuals to keep up will all the primary publications in a few very narrow specialties…\n\nOne frequently needs to repeat the experiments done by another experimenter to establish a protocol new to your lab and some modification of which you intend to use in your own experiment. A micropublication framework also makes it easier to better document these sorts of informal replications performed as precursors to other work it would be very simple to generate a micropublication detailing the replication to provide a better picture of the robustness of the result.\n\n\n\n\nGroth, Paul, Andrew Gibson, and Jan Velterop. 2010. “The Anatomy of a Nanopublication.” Information Services & Use 30 (1-2): 51–56. https://doi.org/10.3233/ISU-2010-0613.\n\n\nPlavén-Sigray, Pontus, Granville James Matheson, Björn Christian Schiffler, and William Hedley Thompson. 2017. “The Readability of Scientific Texts Is Decreasing over Time.” eLife 6 (September). https://doi.org/10.7554/elife.27725."
  },
  {
    "objectID": "002-units_of_publication.html#footnotes",
    "href": "002-units_of_publication.html#footnotes",
    "title": "3  Units of publication",
    "section": "",
    "text": "In addition current nano-publication infrastructure needs a lot of work before it will be a smooth easy-to-use experience for most would-be authors and I suspect also some major performance optimizations would be needed for handling large datasets.↩︎"
  },
  {
    "objectID": "003-Incentives.html#the-old-funding-models",
    "href": "003-Incentives.html#the-old-funding-models",
    "title": "4  Incentives",
    "section": "4.1 The Old Funding models",
    "text": "4.1 The Old Funding models\nThis section is a bit polemical and here mostly for those outside of academia who need the context for the system I’m hoping to disrupt\n\n4.1.1 Pathologies of the Subscription Model in the Internet Era\nFor those unfamiliar with the current problems in academic publishing\nThe classic funding model for an academic journal was the subscription model where a university library, and/or department held a subscription to the journals that the university’s staff wanted to read. These subscription fees paid for the administration and printing of the journal. In the era of physical publication large scale systematic infringement on a journal’s copyright would be expensive, requiring a printing press and relatively easy to enforce. With the advent of the digital age journals ran into the now all too familiar problems of business models that relied on the scarcity of a physical medium that is now practically infinitely replicable at negligible cost.\nNow anyone with an internet connection can have a distribution network approximately equal to that of the official source and copyright is unenforceable in practice with a lot of unpleasant side effects arising from attempts to make it so. Like most businesses they engaged in the attempt keep the same business model by making an infinitely replicable digital artifact artificially scarce with pay walls as well as other rather futile attempts to deny the new reality. Now rendered effectively both non-excludable and non-rivalrous electronic academic papers became a true economic public good in a way that physical copies were not. All attempts to deny this development and return to the undesirable condition of scarcity have been technically and/or politically dangerous. New business models which do not depend on the scarcity of access to papers must replace the old way of doing things, if we are to get back to a reasonable academic publishing market.\nThe loss of profitability of smaller journals resulted in increased consolidation of the publishing market into a small number of large concerns. These larger publishing houses had some economies of scale, and usually some prestige publications still with print versions and which commanded a premium subscription. These large publishing houses made the most of the shield provided against real competition by the grant of ‘limited’ monopoly afforded to them by nation states in the form copyright law and the threat of it’s enforcement, a threat which has claimed at least one life Aaron Swartz. Note that this is a copyright held by the journal on content which they have only a minor role in creating. It is scientists, usually publicly funded, that write and review these articles. The journal provides basic quality pre-screening, matchmaking between authors and reviewers, and if you are lucky copy editing. Placed in a position of oligopoly power the publishing houses were, and still are to a significant degree, able to dictate terms to the university libraries which purchase subscriptions. This has opened the door to classic oligopolisitic practices such as bundling high quality journals in with poor quality ones, a pattern that will be familiar to anyone unfortunate enough to have had to engage with a company selling what the Americans refer to as ‘cable TV’. The management of these complex subscription packages has been outsourced within arcane academic bureaucracies the absurdity and complexity of which put to shame the worst imaginings of Kafka and Heller. Consequently, the majority of academics who actually use these journals have little to no information about how this works and little hope of providing feedback about the quality of service to their librarians that will have a meaningful impact on purchasing decisions. This decoupling of purchasing decisions from the users of the service contributes to the ability of the publisher to continue to deliver worse services at higher prices.\n\n\n4.1.2 Open Access Won’t Save Us\nTo squash the hopes and dreams of those familiar with the problems of the subscription model but who think open access is a panacea\nDon’t get me wrong open access is unequivocally a step in the right direction with respect to making publicly funded research available to the public. Open Accesss 2020 & Plan S go a long way to addressing this issue they are, however, far from enough to fix the malaise in academic publishing. Alas open access as currently conceived allows the publishers to give up on an expensive losing fight with ‘pirates’, (hats off to captain Alexandra Elbakyan of the good ship sci-hub), and still get paid. Unfortunately the publishers saw this coming, it’s been in the works of years and they’ve had time to devise a cunning plan. The pay to publish model has many of the same problems as the old subscription model. In some cases it makes the actual price paid to publish more transparent to the academics paying it from their grants, but this can quickly fade into the background cost of doing business along with all the other overpriced things we spend grant money on. If deals with publishers to bundle x number of publications from affiliated researchers of the grant awarding body or university in the set of Y journals are not already in the works I’ll eat my hat. Scratch that I looked it up and it’s already happening - I promise I predicted it before I checked, see: Projekt DEAL This provides a convenient return the status quo of extracting large fees from research grants to pay for publishing just by a slightly different bureaucratic channel and with little to no improvement in the incentive to provide a quality service. If anything publishers will gain increased revenue from this having successfully spun it as a concession on their part to move to open access, so that they can charge as much or more in publication fees as they charged in subscriptions in the name of a ‘sustainable’ business model. Because anything less that multi-billion dollar annual profits is clearly unsustainable."
  },
  {
    "objectID": "003-Incentives.html#what-is-the-function-we-want-from-the-publishing-industry",
    "href": "003-Incentives.html#what-is-the-function-we-want-from-the-publishing-industry",
    "title": "4  Incentives",
    "section": "4.2 What is the function we want from the publishing industry?",
    "text": "4.2 What is the function we want from the publishing industry?\n\nWhat do we need from a journal and how can we incentivise that and only that reviewer / author matching prioritisation / curation type setting, proofing plagarism, hosting\n\nIn the current publishing ecosystem incentives are often actively bad and when they are not actively bad they are often pretty neutral with respect to the key outcome of good quality scientific research.\npit of sucess\n\nThe Pit of Success: in stark contrast to a summit, a peak, or a journey across a desert to find victory through many trials and surprises, we want our customers to simply fall into winning practices by using our platform and frameworks. To the extent that we make it easy to get into trouble we fail.\n\nThe problem with Altmetrics: goodhart’s law issue with metrics/impact - resolve through alignment"
  },
  {
    "objectID": "003-Incentives.html#the-review-bounty",
    "href": "003-Incentives.html#the-review-bounty",
    "title": "4  Incentives",
    "section": "4.3 The Review bounty",
    "text": "4.3 The Review bounty\nA new funding model for academic publishing\nI propose a new system for funding academic publications which I call review bounties. If you want to publish a manuscript you don’t pay a publication fee you put up a bounty for it’s review. If you wish to be a publisher you don’t put up a pay-wall and charge an access subscription you take a cut of the review bounty. As an author you take what you want to publish to a pre-print and review server you state how much you are willing to pay to have it reviewed and published and you make ranked choice list of venues you would be interested in publishing your manuscript. Publications then have the option to take you up on overseeing the review and possible publication of your manuscript. They contact reviewers who agree to review the manuscript in exchange for an agreed cut of the review bounty. Reviews are open, not anonymous, as are all payment amounts. This openness is essential to prevent grift as if a journal permits you to simply pay some friends for an easy review their reputation as a venue for rigorous review must suffer.\nThere are a number of interesting variations you can make to this basic model. consider the ‘bug bounty’ in which a portion of the review bounty is held in reserve and anyone who finds an error in the work that is of material relevance to the conclusions of the work can claim the bounty. This can be decided by the senior author, a majority of the authors or a consensus of the publisher and original reviewers. The bug bounty accrues back to the authors, publisher and reviewers over time, rewarding them for making non-obvious errors and incentivizing bug hunters to find errors quickly and thus get larger rewards.\nIn “An Incentive Solution to the Peer Review Problem” (Hauser and Fehr 2007) Marc Hauser & Ernst Fehr propose that reviewers who fail to deliver their reviews on time be penalized by having any papers they subsequently seek to have published at the journal for which their review was late be held up twice as long as they made other authors wait. This proposal may incentivize timely review but punishing tardy reviewers in this way has serious negative externalities, it delays the publication of potentially important work and will disincentivize anyone who has ever been late with a review from publishing in that journal again. This is not likely to be convenient for the journal or the reviewer. Overall this does not seem like good game theory outside of a very narrow scope, when designing incentive systems we must keep in mind that the end goal is better science we can’t optimize solely for punctual reviewers. A solution to this problem in the review bounty system is that a reviewer forfeits the bounty if a review is not provided by the deadline. Then another reviewer can claim it, or the original reviewer can reclaim part of it but only if they now provide their review up-front prior to another reviewer claiming the bounty. The longer an author is kept waiting beyond the deadline the more of their bounty they recoup with the cost coming partly from the publisher to incentivise speedy organisation of the review.\nAnonymity and secrecy in peer review is in my opinion thoroughly over-rated, it can conceal abuses of power like holding up someone else’s paper so you or your mate can publish first as much or more than it provides scope for junior people to critique the work of senior people. Openness increases the reputational risk to senior people of petty behaviour like denying someone a job or grant application because they canned your paper. Make a habit of that sort of thing and people will now notice and hold you to account. If you can’t both respectfully take and dish out an intellectually rigorous critique what are you doing in this line of work?\nIf perspective authors cannot afford a review bounty could also have a croud funding (not just public but potentially grant awarding bodies, foundations etc.) method built into the platform to allow the sponsorship of pre-prints to afford enough of a bounty to get reviewed and published. Option a portion of bounty only payable if the rest of the minimum value is met by others and reviewers secured.\n\n4.3.1 poor incentives for quality publishing\n\nformatting\n\ngeneral friction around formatting file types and styling\ncatering to pagination and the dead tree\nfigure panels\nlack of useful animation/interactivity\n\ndeclining readability\n\nScientific manuscripts are getting harder to read (Plaven-Sigray2017?)\n\nCitation Dilution (the bigger a paper the less specific you are being when you reference it) ~ credit dilution / attribution?\nhttps://www.pnas.org/content/118/41/e2021636118\nscope beyond reviewer competence\n\nparticularly in biology the are massive papers published whith 10 years ago would have taken 10 years to complete can now be a sinlge complec paper\n\nnarative seeking / lack of pre-registration/distinction between the predicted and the post hoc\ncommitment to metaresearch to improve research publications\nA public benefit corporation like model or equivolent corporate structure to help prevent a return to the current rachet\n\nreview bounties as a key component of the attracting quality reviewer time away from established publishers\nphysics archive - decoupling of publishing from archiv, pre-prints don’t solve the problem need a new hybrid solution\nSome have made the case that pre-prints are all we need and we can simply cut out the middle man of publishers altogether. Making the case that we can and rely on post-publication review for quality assurance and potentially on algorithmic methods for curation. Most, however see that the journals do serve some curatorial and quality filtering roles, as well as providing an organizing function for arranging that peer-review actually take place. The problem is that under the current economic model the big publishing houses are in a position to act extractively and to seek rents on their oligopoly of prestige publishing, whilst providing a pretty poor quality of service to the academics who are ostensibly their customers. The value that they provide is substantially offset by this behaviour which arises from some poorly aligned incentives. This misalignment is costing billions often in public research money that could be much better allocated if their were a healthier market in publication outlets."
  },
  {
    "objectID": "003-Incentives.html#x",
    "href": "003-Incentives.html#x",
    "title": "4  Incentives",
    "section": "4.4 x",
    "text": "4.4 x\nAt the moment it is reasonably expensive in both time and money to get a publication in a high prestige journal. Getting such a publication constitutes a reasonably strong and reliable signal that you have put in quite a lot of work to the process or trying to get your paper published. This however is not perfectly\ncostly signaling theory in publishing - jumping through arbitrary hoops to signal prestige? make it harder to fake signals, gaming metrics/acceptance with low cost noise that sounds good but lacks substance, stronger openness norms and smaller units give this bad quality signal less space to hide\nproviding your analysis as a literate programming artifact is a fairly high cost signal that is hard to fake, it also has much lower verification costs for 3rd parties. I can just go and look at the code underpinning your paper and see if what you did is good before I would have to ask you, coming from someone you don’t know a request to look over your exact methods\n? croud funded news and views bounties? pol.is and quadratic voting for review/synthesis subject selection\nsuch a platform has the potential to create a transparent competative market for publication services especially if aided by regulatory or other institutional interventions that penalise the predatory business practices of current industry players\nThe authors of the The tragedy of the reviewer commons (https://doi.org/10.1111/j.1461-0248.2008.01276.x) (Hochberg et al. 2009) identify as an issue: “attempting to publish the smallest acceptable unit” this appears to conflict with proposals for ‘micropublications’ or other smaller units of publication but I would argue that the underlying concerns that lead to calling this an issue arise from conceiving as a ‘conventional’ paper as the minimum viable unit of unit publication.\nsteven novella - minimum publishable unit vs minimal unit of publication, emphasis publishable several crappy experiments and a story, if experiment have to stand on their own they will need to be of higher quality not obfucated by being strung together in a narrative.\nsee also sciencematters.io - ‘single observation publishing’\nsee the science breaker (universite de geneve) lay summaries / news and views commision (bounties from anyone?)\nreducing the size of the unit of publication from a full paper to a single experiment, dataset, protocol description, experimental design, theoretical model etc. This makes it easier to do your best work because the scope of the task is smaller, it also make it easier to review effectively\nIf we conceive of the minimum unit of scientific publication as what has become a conventional paper with several often complex experiments and an overarching narrative then an underdeveloped paper can indeed be burdensome to review. We have a compounding problem of increasing technical complexity of papers due to in part rising expectations in the amount of work going into a paper to make it worthy of a ‘top tier’ journal and increasing volume of articles.\nThe authors of ‘The tragedy of the reviewer commons’ (Hochberg et al. 2009) pre-review by colleagues - do this on platform and don’t waste time circulating a bunch of copies by email and reconciling the resulting comments, have and invited reviewers feature that lets you invite input on your work prior to submitting it for formal review\nreviewing a modern paper is a daunting task - uncertainty about the expertise of the other reviewers what is the subset of things in the paper which you are expected to the expert reviewer on are the areas outside your scope adaquately covered by other reviewers? - I full expect people often tacitly assume the editors have handled this adequately as it theoretically their responsibility and just review what they feel comfortable reviewing.\nmedium/form mismatch - way to much for the short format paper\n\n\n\n\nHauser, Marc, and Ernst Fehr. 2007. “An Incentive Solution to the Peer Review Problem.” PLoS Biology 5 (4): e107. https://doi.org/10.1371/journal.pbio.0050107.\n\n\nHochberg, Michael E., Jonathan M. Chase, Nicholas J. Gotelli, Alan Hastings, and Shahid Naeem. 2009. “The Tragedy of the Reviewer Commons*.” Ecology Letters 12 (1): 2–4. https://doi.org/10.1111/j.1461-0248.2008.01276.x."
  },
  {
    "objectID": "004-Medium_matters.html#we-need-to-talk-about-multi-panel-figures",
    "href": "004-Medium_matters.html#we-need-to-talk-about-multi-panel-figures",
    "title": "5  Medium Matters",
    "section": "5.1 We need to talk about Multi-panel figures",
    "text": "5.1 We need to talk about Multi-panel figures\n(Apologies if my personal loathing of multi-panel figure comes through overmuch in the style and tone here, I tried to exercise some restraint)\nWhat information do you usually need to interpret a panel in a figure?\n\nThe figure itself\nThe figure legend\nThe place in the text which references the figure\nmore often than not at least one disambiguation of a novel acronym\n\nEach of these 4 pieces of information can be located on different pages in a modern manuscript. Yes the figure and it’s legend are sometimes of different pages (I’m looking at you Cell WTF?). To drive this point home you can be looking at a situation like this: Figure 6i was discussed on page 4, printed on page 5 and the legend for which is, for some ungodly reason, printed on page 6! Oh and also the title for the figure is not on the figure but in the legend and helpfully features (YATA) yet another terrible acronym that was defined once in the abstract on page 1.\nI don’t know about you but a mere mortal such as myself needs some working memory spare to think about the underlying concepts that a manuscript is trying to communicate to me, and I do not appreciate having to cache as many a 4 look-up tables in my head just to be able to effectively read a figure, let alone reason further about it!\nI content that this problem is worse when looking at electronic representations of formats meant to be printed, at least in part because it is easier to form a mental map of the relationships between these different parts of the manuscript when it is printed. When you have a physical copy the parts have relationships to one another in 3D space that take less effort to construct a mental model of. Also it’s just easier to arrange the pages so can actually look at the different parts and the same time. e-reader like formats also suffer from this issue as they don’t have fixed pagination so the relationships between objects are harder to model still as they are not even in fixed relationship to one another in a virtual 2D space.\nI hypothesize that breaking up figure panels placing information, such as titles and abbreviations unnecessarily relegated to figure legends in the figure itself and placing figures in line in the text as they occur would increase reading speed and comprehension, especially in electronic media. I term this information-non locality minimization.\nSpecific hypotheses I’d like to see tested:\n\nTime taken to read a paper with multi-panel figures will be longer than time taken when information-non locality is minimised.\nReading comprehension tests will be worse for papers with multi-panel figures than when information-non locality is minimised.\nHypotheses 1 & 2 will be true of both electronic and physical media but the effect size will be larger for electronic media.\nBound copies of papers with multi-panel figures will provide less of an advantage over information-non locality minimised copies than loose leaf / unbound prints.\nEffect size of the advantage information-non locality minimized versions have over multi-panel figure versions will be larger in people with dyslexia. (I’m dyslexic and it’s been thought of as an issue related to working memory)\n\nIf anyone wants to do this research get in touch I might even be willing to contribute to funding it.\nTo sum up Multi-panel figures are an anachronistic concession to typesetting color prints to a compact format for printing and they have no place in modern electronic publishing. They ruin the flow of reading a paper. They represents an unreasonable assault on the working memory resources of a reader who is trying to understand what is likely a complex topic and create undue cognitive load for readers who need their full mental faculties about them to graple with complex scientific ideas.\nAdditionally I think that they encourage bad graphical practices. Panels are labeled not with meaningfully interpretable titles as both our schooling and basic common sense about presenting information dictates they should be but with alphabet soup. These are bad practices for which we take school children to task for in science classes but seem to have collectively forgotten in the peer reviewed literature. Multi-panel figures should be used ONLY when it is actually useful for the understanding of the content for visuals to be placed together. We don’t need to optimize for printing in electronic formats we can have as many full size color figures as we need to optimally convey our point. This would be an improvement even if we stick to a pdf as the primary end point paradigm which I will be making a case against later.\n(Rant over 😉 )\nWe will now be asking, what can you get when you ditch the static pdf as the primary end point?"
  },
  {
    "objectID": "004-Medium_matters.html#interact",
    "href": "004-Medium_matters.html#interact",
    "title": "5  Medium Matters",
    "section": "5.2 Interactivity, Animation, & More",
    "text": "5.2 Interactivity, Animation, & More\nThere is some skepticism over the value of interactivity and especially animation in actually being effective at communicating usefully interpetable scientific information. Just as there a many ways to produce terrible uninformative graphs there are many ways for interactivity and animation to be ineffectually employed but there are a number of areas in which they excel at effective communication in ways not easily captured in static plots.\nTo make the point about interactivity I will largely let this little widget from canvasXpress make my point for me. Imagine ever plot in a scientific paper worked like the widget below. No seriously make it full screen and play around with it a bit, right click in the plot and take a look at the options. Look at the code I wrote to get this level of functionality, I’ve left it visible above the plot - yes that’s it.\n\nlibrary(canvasXpress)\n# TODO: make a better example plot\ncanvasXpress::canvasXpress(mtcars)\n\n\n\n\n\nI’m serious really play with it before you move on, have you tired double clicking, have you looked in configure?\nThis sort of capability is particularly valuable for looking at publications of exploratory analyses where the authors publishing the work\nLet’s Consider some other uses of interactivity. You know those ‘representative images’ people put in their papers when they do a bulk analysis of images what if the representative image was a widget that picked one at random from the appropriate group in the dataset deposited at the Image Data Archive and changed it periodically? A norm like this might have a positive effect on the representativeness of those images don’t you think?\n\n5.2.1 The utility of animation for representing uncertainty"
  },
  {
    "objectID": "004-Medium_matters.html#objections-to-dropping-pdfs",
    "href": "004-Medium_matters.html#objections-to-dropping-pdfs",
    "title": "5  Medium Matters",
    "section": "5.3 Objections to dropping pdfs",
    "text": "5.3 Objections to dropping pdfs\n“We can’t get rid of pdfs we need a version of the paper that we can archive!” I hear you cry. Yes, we do archival formats are important but the archival format does not have to be prettily typeset to minimize the number of color prints there are archival formats as good or better than pdfs\nIn Section @ref(computational-irreproducibility) Computational Irreproducibility & In Section @ref(key-technologies) Key Technologies I will be making the case that a form of plain text document should be the definitive reference version, key end point and primary authoring format.\nThe separation of semantic content from formatting information (css / html) why plain text? parseability - interoperability with many downstream text processing tools small, version control, temporal stability of format, open"
  },
  {
    "objectID": "005-Computational-irreproducibility.html#computational-transparency-or-lack-thereof",
    "href": "005-Computational-irreproducibility.html#computational-transparency-or-lack-thereof",
    "title": "6  Computational Pitfalls",
    "section": "6.1 Computational Transparency (or lack thereof)",
    "text": "6.1 Computational Transparency (or lack thereof)\nVery few papers published today don’t contain at least one computer generated graph and a statistical test. Computing is ubiquitous in research and growing ever more so. The chances are therefore, that somewhere in any given paper you publish there is a number generated from your data by a computer. Now there are plenty of sources of error that can be introduced between your data and the number that ends up in your paper. These can be simple errors like typos and bugs in software, conceptual errors like using the wrong tool and a variety of other failure modes. We can with some of the technologies I outline in the next section eliminate many of the sources of simple error and ensure that any conceptual mistakes are at least properly documented so that we can tell if we for them look carefully. My goal for the new publishing medium is that the published artifact should provide a complete description of every function applied to the input data to produce the outputs. That is to say if a number or plot appears in your manuscript then the published artifact should show how that was derived from your data.\nIn the most minimal example consider I want to do some arithmetic, why should I ask you to trust me that I did the calculation right when I can write the calculation out and have the computer evaluate it? If I want to tell you the value of x is r round(7957/3) but the fact that I computed it by doing \\(x=\\frac{7957}{3}\\) does not need to be in the manuscript at this point I can put the computation specification in the source document that generates the manuscript. In the case for the number above it is a piece of code that looks like this:\n` r round(7957/3) `\nThat way you can check my work and I can’t accidentally transpose a number copying the output into my manuscript. I can still introduce bad data (garbage in garbage out) and I can still specify and incorrect computation but now you can see what the computation is and if I misspecified it. This generalizes from the simple case to complete complex computational pipelines and tools to make this quite achievable in practice now exist as we will see later.\nThis might seem like quite a bit of trouble to go to, why am I making this rather exacting auditability a priority for the future of scientific publications? We’ll see in the next section."
  },
  {
    "objectID": "005-Computational-irreproducibility.html#computational-irreproducibility",
    "href": "005-Computational-irreproducibility.html#computational-irreproducibility",
    "title": "6  Computational Pitfalls",
    "section": "6.2 Computational Irreproducibility",
    "text": "6.2 Computational Irreproducibility\nIt is hard to reproduce, not to replicate but merely to exactly redo analyses published in many papers. Complex bioinformatic analyses can take months to reproduce and often simply cannot be done without contacting their authors for additional information (Garijo et al. 2013). Much progress on this issue has been made, (by some), since the publication of that study however best practices are under-defined, inconsistently applied, not taught well/widely and not well incentivised. This latter point is very important there is little incentive when publishing to ensure computational reproducibility. Few reviewers will flag this as an issue. Also we are quite accustomed to seeing phrases like “analysis was performed with custom in-house python scripts”. This should worry us, I’ll be expanding a little on why below. I’ve been guilty of this sort of vagueness before I learned some of the tools to make it practical to avoid this. These scripts may or may not be made available with the manuscript and if they are it may be impossible to get them to run if details of the computational environment like software versions are not also provided. Without details of the computational environment there is also no guarantee that even if it runs it will produce exactly the same output. This can be at multiple levels from package versions you are using to the OS you are running. find the reference for the bioinf tool that produced different results on macOS and linux.\nThis would be bad enough but software has bugs, lots of bugs. Highly paid teams of professional programmers with years of experience and automated testing frameworks pump out software used by millions of people that is full of bugs. There are plenty of bugs in software written for highly regulated industries with exacting safety standards like the military, aviation, and medical devices. Some of these bugs have killed people, others have cost companies millions (cite humble pi) The key point is that highly motivate professionals find it impossible to eliminate all bugs even when lives depend on it. Most code written by academics for the purposes of doing data analyses is not subject to much scrutiny it rarely has to survive a ‘production environment’ also much of it, no offense, is written by semi-self taught amateurs. How buggy is what we write likely to be when we don’t have nearly as rigorous a process or nearly as much exposure to real world stresses to reveal errors as professionally written code? This applies to us all even if you don’t write what is conventionally thought of as code. Ever used and excel function? congratulations you written some code and it might be buggy.\nNow there is good reason for us academics to not expend vast efforts on the sort of rigorous software engineering someone like NASA would do, and I’m not saying we should. We need to be able to prototype rapidly, to make horrendously non-performant proofs of principle, to hack things together, make things so specialized that they only need to run for our specific problem and it doesn’t matter terribly if they are slow or not user friendly.\nWe do however need to be able to check our work, and that of our colleagues. We need an audit trail of what we did and how so that we can track down and understand sources of error. The same way that we keep detailed lab notebooks that can help us spot mistakes and improve protocols at the bench, we need analogous processes for our computational work. let’s return to excel, have you ever sorted by a column in excel and got the dialog box asking if you want to expand your selection to all columns or just re-order the one you selected? If you picked the latter option congratulations you just effectively randomized the order of one column with respect to the others. It is quite easy to do this and it can be quite hard to tell if you have if you don’t notice immediately. beyond the ephemeral undo history there is also no record of this operation stored in excel so you can’t easily do some forensics to figure out where it all went wrong. This is a reason to prefer writing code to working with a GUI without a detailed and persistent history of the operations you have performed in a form that you can share with others and use to repeat your analysis exactly.\nWhen working with software are working with high level abstractions that crystallize an immense body of understanding often opaque to us as individuals behind the click of a button or a function call. It is thus essential to the verifiability (Hinsen 2018) of our work that we can document how we used these tools so that others with appropriate expertise can assess the technical correctness of the tool and the validity of our use of it for the question at hand.\nAs we become more specialized and the paths of inference from observations to conclusions become longer and more convoluted than can fit in the scope of our individual knowledge. As a result we must strengthen the approaches we take to insuring correctness of the chain of inferences at the weak points where is passes from one domain of specialized knowledge to another.\nThis lack of reproducibility need not be the case, we have the technical means to overcome this issue with tools primarily originating in the modern software development and deployment industry. In the next section I will be covering how we can make use of some of these technologies to overcome some of the pitfalls outlined here and even to make the process of writing a publishing research a more streamlined, less painful and more rigorous process than the way be do it now.\n\n\n\n\nGarijo, Daniel, Sarah Kinnings, Li Xie, Lei Xie, Yinliang Zhang, Philip E. Bourne, and Yolanda Gil. 2013. “Quantifying Reproducibility in Computational Biology: The Case of the Tuberculosis Drugome.” Edited by Christos A. Ouzounis. PLoS ONE 8 (11): e80278. https://doi.org/10.1371/journal.pone.0080278.\n\n\nHinsen, Konrad. 2018. “Verifiability in Computer-Aided Research: The Role of Digital Scientific Notations at the Human-Computer Interface.” PeerJ Computer Science 4 (July): e158. https://doi.org/10.7717/peerj-cs.158."
  },
  {
    "objectID": "006-Key_technologies.html#literate-programming",
    "href": "006-Key_technologies.html#literate-programming",
    "title": "7  Key Technologies",
    "section": "7.1 Literate Programming",
    "text": "7.1 Literate Programming\nThe case for writing in plain text \nWhy would I write in a plain text file when I could use something like word or google docs? I used to be a word aficionado I new all the fancy features, and when real-time collaborative editing came along in google docs I was on-board and longing for reference management integrations. Now I write almost exclusively in plain text files, I can still do all the same stuff I did in word and google docs but also much more. I do this even when I’m not coding I wrote my thesis like this, I’m doing it now. I can work in an editor that is a bit more like word or google docs in appearance but because I often write code mixed in with my prose I generally don’t. I’m using a simple set of rules for formatting my text document called markdown.\nHere is ~98% of all the markdown syntax you are likely to need to know:\n# Heading 1\n\n## Heading 2\n\n## Heading 3 {#h3}\n\na reference to a section \\@ref(h3)\n\n- bullet point\n- another point\n\n1. *italics!*\n2. __Bold__\n\n[hyperlink](https://en.wikipedia.org/wiki/Hyperlink)\n\nImages: [!Alt text](path/to/image)\n\nan inline reference to @key123 or a parenthasized one [@key123]\n\ninline `code`\n\n```\n# A block of code\nprint(\"Hello, World!\")\n\n```\n\nIf using mathjax or a simialar tool: inline latex math: $\\frac{x}{y}$\n\nEquations:\n\n$$\na=\\frac{x}{y^2}\n$$\nPretty simple right? If you can learn to use some of the slightly more advanced features of MS Word like styles, sections, track changes or automated reference management you can learn markdown pretty easily. Also there are now some quite nice visual editors to give you more of the WYSIWYG editing experience if you need a leg-up with the syntax, there is one built right into the tool I’m authoring this document in right now RStudio. It even has a nice feature that will let me insert references from my Zotero library or by DOI search and add them to the bibliography for this bookdown project.\nBut why do I do this? for one reason as I write this it is being rendered every time this file is saved to a window I have open on my second monitor so I can see what you will be seeing. That is if you are reading this on the website because I’m also generating an epub and pdf document from my source text. If I felt so inclined I could add a few lines of configuration and also generate a word version, or use I could use rticles to format it in the style of a paper from my favorite journal. I could also try for various forms of slideshow including powerpoint but this is a bit verbose for slides. The point is you get a lot of different output options from the same starting point. This is because writing this way makes use of a key insight that is now central to good web design, the separation of the semantic content from how it is formatted.\nIf like me you are old enough to have been taught to write webpages in school in the early 2000’s or to have had a myspace page you may remember setting formatting rules for text in html tags. Setting the color of some text in an html tag is now anathema to the web designer, formatting is done with css not in the html! Why? Because you can completely change the look, layout, navigation or the way you display the same content if you structure things this way, that’s why I’m able to render this document in three different formats at a keystroke. Its semantic structure is governed by the very simple rules outline above, from that this text can be parsed into a data structure that permits it to be formatted and output in a myriad different ways. This also allows for much improved accessibility as a sane order for screen readers inherent in the structure of the source document and what ever size and typeface you’d like this represented in is up to you. As well as easy text mining and searching that does not require wrestling this text back out of a pdf into a nice structure for computers to work with.\nAlso as I alluded to above I often write a mixture of prose and code this is called Literate Programming where the documentation to my code and my description of what it does is interwoven with the code. It is from this concept that I get the title of this piece “Literate science” my proposition is that we should move to a paradigm of authoring academic manuscripts that resembles literate programming. To be clear I’m not suggesting that everyone learns to code nor that our manuscripts should be detailed documentation of code. What I am suggesting is that the source document that generates your manuscript should generate every number and figure in that manuscript from your input data. You shouldn’t need to learn to code to do this. You may, however, need to use tools which generate code which is inserted into the source document for your manuscript, instead of using tools where there is no means of exactly reproducing the steps you took in the analysis you performed with a gui tool (see section @ref(computational-irreproducibility)). Remember that interactive plot I told you to play with back in section @ref(interact) on in\n1"
  },
  {
    "objectID": "006-Key_technologies.html#collaboration-and-version-control-with-git",
    "href": "006-Key_technologies.html#collaboration-and-version-control-with-git",
    "title": "7  Key Technologies",
    "section": "7.2 Collaboration and version control with git",
    "text": "7.2 Collaboration and version control with git\ngit is a tool used by programmers to keep track of and navigate through the history of changes they have made to code, also to collaborate on code. Originally developed by Linus Torvalds to facilitate the distributed development of the Linux kernel this tool can be applied as well to any body of text as is can to computer source code. This document I’m writing now is in a git repository. It is keeping track of the differences between what is in the files I’m editing now and the last time I ‘commited’ my previous set of changes. With it I can navigate through all my past changes, and revert to previous versions should I choose to. Each commit is associated with a unique hash so if you needed to cite a very specific version of this document you could identify the commit hash associated with the exact version where the change you are interested in was made. This is a very useful feature of git for the purposes of academic publishing, the ability to cite a repository and a specific version of that repository permits us to handle updates and retractions much more smoothly. If you go to a\nI can collaborate on this text asynchronously with others using git. If they ‘fork’ off their own ‘branch’ of the project and make some changes we can ‘merge’ them back together looking at all the places our versions differ and choosing which to keep. It also keeps track of who authored which parts of the text, a potentially useful feature for the attribution of authorship credit but with the clear limitation of capturing only who actually wrote the text. One could also do google docs style real-time collaborative editing of your source document and then commit the resultant changes with git using a tool such as hackmd.io.\ngit has a fairly rich and complex feature set but these are the key ones for our purposes and they can all be accomplished with fairly easy to use gui interfaces such as those implemented at github and git lab. However a git interface tailored to the needs of document authors and not explicitly software development would be a useful addition to the ecosystem and something I would want to see a a part of the platform I’m proposing."
  },
  {
    "objectID": "006-Key_technologies.html#containers-and-build-pipelines",
    "href": "006-Key_technologies.html#containers-and-build-pipelines",
    "title": "7  Key Technologies",
    "section": "7.3 Containers and build pipelines",
    "text": "7.3 Containers and build pipelines\ngit and gitlab for versioning in collaborative plaintext document editing markdown as a simple to use plaintext markup lanugage Rmarkdown/kintr/pandoc with Rstudio as document editor/renderer literate programming gitlab ci-cd / docker for reproducible computational environments and automated checking automatically see if your edit to a manuscript if that change breaks a formatting rule automated QC checking and helping, readability analysis, plagarism, stats checks etc.\nhttps://www.nature.com/articles/d41586-021-03807-6"
  },
  {
    "objectID": "006-Key_technologies.html#why-not-blockchain-yet",
    "href": "006-Key_technologies.html#why-not-blockchain-yet",
    "title": "7  Key Technologies",
    "section": "7.4 Why not Blockchain (yet)?",
    "text": "7.4 Why not Blockchain (yet)?\nYou might want to skip this if your not a crytpo person, it’s mostly to explain to people on the crypto train while I’m not on board yet for this project\nAll this stuff about putting up a bounty, being able to claim that bounty, having amounts of money accrue back to certain parties, doing compute, covering the cost of compute, being open and public sounds like the sort of thing that would gel really interestingly with blockchain with smart contracts and distributed compute? - I hear you ask. Yes, I answer, but the tooling is not there and the ecosystem is not yet mature enough to support what I am proposing here. most of tech stack to support my proposals exists in stuff that was not built to talk to blockchains. I expect to see a version of what I propose built on blockchain technology in the future but I anticipate a timescale on the order of decades. In addition It would be a barrier to the wider adoption of my proposed approach until such time as crypto has gained wider acceptance and understanding. Like it or not justified or not crypto has in the minds of many some negative connotations. I’m already asking for quite a big jump from people crypto is like a bridge to far.\nThere are many potential advantages: Cyptographically backed identity management for authors, very robust provenance of ideas in a public record, preventing the raising of barriers to knowledge or censorship by governmental of corporate entities through its decentralized nature, etc. A strong crytographic base might useful for the implementation of certain security tools such as limiting data asses to approved individuals with verified credentials and access rights. e.g. allowing access to data only if an individual has an api key signed by by several parties such as their personal identification token to demonstrate it is them and by the tokens of an ethics review board. It may also be possible to do things like run analyses on data to which the researcher is never granted access directly and the code to run the analysis has to be signed by a technical review board who have checked the code is not able to exfiltrate the data it is given access to for analysis.\nThe ability to transact and have ‘smart contracts’*, to govern the review bounty process does provide a potential upside of this tech especially when it comes to dealing with potential regulatory hurdles associated with this. Any situation where you are holding money which you may at some point be obliged to give back to another party tends to attract a lot of red tape. Cardano is I think the most likely candidate in the current crypto market to be the basis for such a system given it’s technical characteristics and its governance structure’s fondness for peer-review. (Declaration of interest: I have some modest investments in cardano, etherium, bitcoin and monero; cardano having the largest share)\nHowever in short almost all of the aims I outline here and even many of those stated by people trying to deploy blockchain tech in this space, such as DEIP & VitaDAO, can be achieved without the use of blockchain technology. Things like author contribution metrics tied to git commits, (imperfect as these would be as a representation of relative contribution), and reputation metrics associated with an author’s ID, even crytographically verifiable IDs, can be accomplished with a PGP web of trust a yubikey and existing git tools like github/gitlab. Even much of the decentralized model is achievable through a federated architecture, don’t get me wrong I’m bullish on crypto I just don’t think the layer 2/3 tech that I think this approach to publishing would need to be blockchain-native is ready for prime-time just yet. Happy to be convinced otherwise, but I’m going to want to see some running code.\n*This is a terrible name BTW, automated contracts would be a lot more accurate, basic conditional logic != smart."
  },
  {
    "objectID": "006-Key_technologies.html#footnotes",
    "href": "006-Key_technologies.html#footnotes",
    "title": "7  Key Technologies",
    "section": "",
    "text": "NB Markdown’s competitors that largely lost the format wars reStructuredText (rST) and AsciiDoc would arguably be a better choice for the text-based lingua franka format of academic publication going forward but the tooling and ecosystem around these is just not quite as mature.↩︎"
  },
  {
    "objectID": "007-The_platform.html",
    "href": "007-The_platform.html",
    "title": "8  The platform",
    "section": "",
    "text": "To achieve this I propose the following: - New commercial publications and publication platforms should be incorporated using ‘public benefit’ or ‘social purpose’ like corporate structure and not conventional ‘C’ corporations. (I’m working here with my limited knowledge of US corporate structures, not all jurisdictions even, as I understand it, in the US have these types of corporate entities yet). It is my understanding that under US law publicly traded companies are under an fiduciary duty to maximise shareholder profit lest they expose themselves to liability for lost earnings by their shareholders, with private equity carrying the expectation of eventually going public making back the investment. This produces a misalignment between corporate entities and their customers with the company incentivised to attempt to exploit their existing customer base for short term stock price gains instead of entering into a long term positive sum interaction with their customers and serving the intended purpose of the corporation to provide the services of an academic publisher. A ‘B corp’ on the other hand is given license to prioritize its purpose over maximizing its bottom line. - We base new publications on open source software stacks. (by which I actually mean free/libre software to appease (a fellow Richard)[fsf.org] and fellow pedant). I propose the development of open publication platforms by ‘B corps’ (see above).\nThe business model I envision for them is similar to that employed by many open source software based businesses such as RedHat and SUSE. Individual journals would pay for support for their own instances, consulting, some bespoke development for their needs that will be committed to the public code base and SaaS (software as a service) style hosting for groups who want and instance without the technical overhead. The SaaS service must always have feature parity with the community version.\nInstances would be part of a federated network with the ability to use accounts on one instance to interact with another instance akin to matrix, mastadon or peertube and likely employing OCRID for universal academic identification.\nIt is essential that such a publishing platform be built on a free software license this permits the publisher to pre-commit to a business model in which they cannot treat their customers unethically without risking incurring significant costs. With a publishing platform based on such a license a company that develops it must continue to add value or it faces a credible threat that the codebase could be forked by a competitor or a community effort. As such as platform would offer it services over a network it should ideally make use of a license which protects against loss of free software protections by offering the software as a service such as the AGPL. licensing under the AGPL and taking community contributions without a contributor license agreement (CLA), by which contributor grant the company the copyright to their individual contributions, serves as an extremely effective pre-commitment strategy to keeping the software free as the software cannot then be re-licensed without the consent of all contributors.\n\nformative peer review https://publicphilosophyjournal.org/overview (lessons from plants)\n\n\nfederated pre-print pool in addition to AI pulicatoin matching - similarity metric for your text to profile for a journal Publication could submit an offer to review a preprint (offer dicounts for choosing them?)\n\n\nuniversial review transfer between possible publications on the platform\n\nAn open review process where a submitted manuscript is a pre-print and a formative peer review process may or may not result in it becoming a formally peer review process and which enables collaborations when additional work is needed to to to a publication quality peice of work\nfederated article submission platform where you submit not to a single journal to multiple ones - I’m still thinking about the incentive structure engineering on this last point possibilities include journals bidding on articles with reduced fees, more reviewer time, authors providing ranked wight of first refusal list and voting systems to help rank interesting candidates.\nit remains the case that automated reference management does not play well with collaborative document editing - versioning and track changes hell\nEach journal can specify things such as how long each section is allowed to be with hard and soft limits, the same applies to the number of figures and references"
  },
  {
    "objectID": "008-Adoption.html",
    "href": "008-Adoption.html",
    "title": "9  Adoption",
    "section": "",
    "text": "Out-competing the incumbents on community trust\nIn order for new companies starting out in the publication space to succeed in addressing the issues that have arisen from the extractive behaviours of the current publishing racket/oligopoly they must out compete the conventional publishing houses in a number of dimensions. They must do so not merely in ease of use, novel features and scholarly merit, but also in the trust of the academic community that they are not going to become the very thing they sought to destroy and dash our hopes of lasting and effective reform of academic publishing. The academic community must be able to have confidence that the new journals will not be captured by the industry incumbents and lead to the same old problems recurring. Look how Mendeley stagnated after the Elsevier aquisition and began introducing anti-features like encrypting local databases reducing user data probability.\nPrioritise features that will make it appealing to use for authors/reviewers - editing UI, collaboration tools, reference management - directly apply reviewer editor changes to fix e.g. typos as PRs - needs guidance for how best to use this for all, build in onboarding - journal template linting - this is a unique feature -\nA new company starting out in this space\nfederated network based on an open source technology\nthe federated model permits\n‘B corp’\nsmall practical benefits\npit of sucsess - raise the bar at the low end to make firm foundations for building peaks but not expecting all to be peak feats\n‘go to market strategy’ starts with applying this concept where the Unique selling point of reproducible computation is valuable and the level of technical knowledge is high so the rough edges on UI and effective onboarding can be handled over time. This starting market is bioinformatics and other disciplines cognisent of the need for computational reproducibility and with some members already comfortable with things like R notebooks. (let me know if you work in field which fits this description) start with a publishing house for such specially journals as an on paper a subsidiary or separate company of the main platform development company and expand into other markets. Academic society journals are in trouble at the moment, they often worked on the closed model and are finding it hard to transition to the open pay to publish model. Their overheads for running a journal are quite high this platform would aim to majorly reduce those administrative overheads, making it easier for smaller groups like academic societies to publish a journal This is another niche to pursue once the initial ease of use kinks are worked out in the more technical communities."
  },
  {
    "objectID": "010-references.html",
    "href": "010-references.html",
    "title": "References",
    "section": "",
    "text": "r if (knitr:::is_html_output()) ' '"
  }
]